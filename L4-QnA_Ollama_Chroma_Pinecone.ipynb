{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38ae2c",
   "metadata": {},
   "source": [
    "## Part 1: Creating the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bb0dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:38:17,359 DEBUG Added a stderr logging handler to logger: urllib3\n",
      "2024-06-10 10:38:17,359 DEBUG Added a stderr logging handler to logger: urllib3\n",
      "2024-06-10 10:38:17,359 DEBUG Added a stderr logging handler to logger: urllib3\n",
      "2024-06-10 10:38:17,359 DEBUG Added a stderr logging handler to logger: urllib3\n",
      "2024-06-10 10:38:17 - DEBUG - __init__.py:add_stderr_logger:90 - Added a stderr logging handler to logger: urllib3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed API Key: ollama\n",
      "Infer API Key: ollama\n",
      "Embed API Base: http://localhost:11434/v1\n",
      "Infer API Base: http://localhost:11434/v1\n",
      "Embeddings Model: mxbai-embed-large:latest\n",
      "Inference Model: gemma:2b\n",
      "RAG Index Name: langchain-deeplearningai-mxbai-ollama\n",
      "using Chroma Vector database\n"
     ]
    }
   ],
   "source": [
    "useLogging = True # set to True to get logging information (and hopefully track which LLM is called when)\n",
    "use_Ollama_For_Inference = True # set to True to use Ollama inference models (and pull at least the gemma:2b model)\n",
    "use_Ollama_For_Embedding = True # set to True to use Ollama embedddings models (and pull at least the nomic-embed-text:latest model)\n",
    "use_Pinecone = True #Turn on to use a Pinecone database. Sign up at www.pinecone.io for a free plan (including 5 indexes)\n",
    "use_Chroma = True #Turn on to use a local Chroma database. Supersedes the use_Pinecone flag above (and turns it off)\n",
    "use_Test_Data = False # set to True to use LimitedCSVLoader class below and only load the 577th item from the CSV file and test that querying with embeddings work well.\n",
    "\n",
    "import openai\n",
    "#Defaults to OpenAI if use_Ollama_For_Inference=False and use_Ollama_For_Inference=False\n",
    "openai.api_base = inferApiBase = embedApiBase =  \"https://api.openai.com/v1\"\n",
    "openai.base_url = inferBaseUrl = embedBaseUrl = \"https://api.openai.com\"\n",
    "openai.api_key = inferApiKey = embedApiKey = os.environ['OPENAI_API_KEY']\n",
    "embeddings_model_name = \"text-embedding-ada-002\"\n",
    "embeddings_model_name_short = \"ada\"\n",
    "embeddings_vector_size = 1536\n",
    "infer_model_name = \"gpt-3.5-turbo\"\n",
    "llm_platform = \"openai\"\n",
    "embed_chunk_size = 1000\n",
    "embed_overlap = 0\n",
    "\n",
    "\n",
    "if use_Ollama_For_Inference:\n",
    "    inferApiBase = \"http://localhost:11434/v1\"\n",
    "    inferBaseUrl = \"http://localhost:1143\"\n",
    "    inferApiKey = \"ollama\"\n",
    "    infer_model_name = \"gemma:2b\" #you can/should customize this to test different Ollama LLMs. Use the NAME field from `ollama list`\n",
    "\n",
    "\n",
    "if use_Ollama_For_Embedding:\n",
    "    llm_platform = \"ollama\"\n",
    "    embedApiBase = \"http://localhost:11434/v1\"\n",
    "    embedBaseUrl = \"http://localhost:1143\"\n",
    "    embedApiKey = \"ollama\"\n",
    "    embeddings_model_name = \"mxbai-embed-large:latest\"\n",
    "    embeddings_model_name_short = \"mxbai\"\n",
    "    embeddings_vector_size = 1024\n",
    "    embed_chunk_size = 512\n",
    "    embed_overlap = 10\n",
    "    #\n",
    "    #embeddings_model_name = \"nomic-embed-text:latest\" #you can/should customize this to test different Ollama LLMs. Use the NAME field from `ollama list`\n",
    "    #embeddings_model_name_short = \"nomic\"\n",
    "    #embeddings_vector_size = 768\n",
    "    #embed_chunk_size = 8192\n",
    "    #embed_overlap = 0\n",
    "\n",
    "print('Embed API Key:', embedApiKey)\n",
    "print('Infer API Key:', inferApiKey)\n",
    "print('Embed API Base:', embedApiBase)\n",
    "print('Infer API Base:', inferApiBase)\n",
    "print('Embeddings Model:', embeddings_model_name)\n",
    "print('Inference Model:', infer_model_name)\n",
    "\n",
    "\n",
    "index_prefix = \"langchain-deeplearningai-\" + embeddings_model_name_short + \"-\"\n",
    "if use_Test_Data:\n",
    "    index_prefix+='s-'\n",
    "rag_index_name = index_prefix + llm_platform\n",
    "print('RAG Index Name:', rag_index_name)\n",
    "\n",
    "if use_Chroma:\n",
    "    print('using Chroma Vector database')\n",
    "    import chromadb\n",
    "    use_Pinecone = False\n",
    "    storage_path = os.environ.get('CHROMA_STORAGE_PATH')\n",
    "    if storage_path is None:\n",
    "        raise ValueError('CHROMA_STORAGE_PATH environment variable is not set')\n",
    "    \n",
    "\n",
    "elif use_Pinecone:\n",
    "\n",
    "    print('using Pinecone Vector database')\n",
    "    from pinecone import Pinecone\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    from tqdm.autonotebook import tqdm\n",
    "\n",
    "    PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "    PINECONE_ENV = os.environ.get(\"PINECONE_ENV\", \"PINECONE_ENV\")\n",
    "\n",
    "    if PINECONE_API_KEY is None:\n",
    "        raise ValueError(\"PINECONE_API_KEY environment variable not set.\")\n",
    "        # Name our index on Pineconeopenai.api_key\n",
    "\n",
    "    # Init pinecone\n",
    "    pc = Pinecone(\n",
    "        api_key=PINECONE_API_KEY,\n",
    "        source_tag=\"langchain-deeplearningai\"\n",
    "    )\n",
    "else:\n",
    "    print('using In Memory Vector database')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "if use_Ollama_For_Embedding:\n",
    "    embeddings_model = OllamaEmbeddings(model=embeddings_model_name, embed_instruction='', query_instruction='')\n",
    "    #embeddings_model = OllamaEmbeddings(model=embeddings_model_name)\n",
    "else:\n",
    "    embeddings_model = OpenAIEmbeddings(model=embeddings_model_name)\n",
    "\n",
    "if useLogging:\n",
    "    import logging\n",
    "    import requests \n",
    "\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG,\n",
    "                        format='%(asctime)s - %(levelname)s - %(filename)s:%(funcName)s:%(lineno)d - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    requests.packages.urllib3.add_stderr_logger()\n",
    "    OLLAMA_DEBUG=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e370b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "\n",
    "class LimitedCSVLoader(CSVLoader):\n",
    "    def load(self):\n",
    "        # Call the original load method to get all rows\n",
    "        all_rows = super().load()\n",
    "\n",
    "        # Restrict to the first 5 rows\n",
    "        #limited_rows = all_rows[:5]\n",
    "        # Restrict to line #577\n",
    "        limited_rows = all_rows[577]\n",
    "        if isinstance(limited_rows, Document):\n",
    "            limited_rows = [limited_rows]\n",
    "        return limited_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7249846e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file, encoding='utf-8')\n",
    "\n",
    "if use_Test_Data:\n",
    "    loader = LimitedCSVLoader(file_path=file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913c5e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents length: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\r\\n\\r\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\r\\n\\r\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\r\\n\\r\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\r\\n\\r\\nQuestions? Please contact us for any inquiries.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(f'Documents length: {len(docs)}')\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfaba30",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "##pip install docarray\n",
    "##pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e200726",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "rc_text_splitter = RecursiveCharacterTextSplitter(chunk_size=embed_chunk_size, chunk_overlap=embed_overlap)\n",
    "\n",
    "if use_Chroma:\n",
    "    print(f'initializing the Chroma local vector store {rag_index_name}')\n",
    "    from langchain_chroma import Chroma\n",
    "    \n",
    "    rebuildIndex = True\n",
    "    if rebuildIndex:\n",
    "        chromaClient = chromadb.PersistentClient(path=storage_path)\n",
    "        try:\n",
    "            print(f'Deleting collection {rag_index_name}')\n",
    "            chromaClient.delete_collection(name=rag_index_name)\n",
    "        except ValueError as ex:\n",
    "            print(f'Could not delete collection {rag_index_name}')\n",
    "\n",
    "    if not use_Ollama_For_Embedding:\n",
    "        chromaClient = chromadb.PersistentClient(path=storage_path)\n",
    "        collection = chromaClient.get_collection(name=rag_index_name)\n",
    "        if collection.count() <= 0:\n",
    "            index = VectorstoreIndexCreator(\n",
    "                vectorstore_cls=Chroma,\n",
    "                embedding=embeddings_model,\n",
    "                text_splitter=rc_text_splitter,\n",
    "                vectorstore_kwargs={ \"collection_name\": rag_index_name,\n",
    "                                    \"persist_directory\":  storage_path}\n",
    "            ).from_loaders([loader])\n",
    "    else:\n",
    "        chromaClient = chromadb.PersistentClient(path=storage_path)\n",
    "        collection = chromaClient.get_or_create_collection(name=rag_index_name)\n",
    "        if collection.count() <= 0:\n",
    "            print('Creating Chroma database using Ollama embeddings')\n",
    "            index = VectorstoreIndexCreator(\n",
    "                vectorstore_cls=Chroma,\n",
    "                embedding=embeddings_model,\n",
    "                text_splitter=rc_text_splitter,\n",
    "                vectorstore_kwargs={ \"collection_name\": rag_index_name,\n",
    "                                    \"persist_directory\":  storage_path}\n",
    "            ).from_loaders([loader])\n",
    "            # from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "            # text_splitter = RecursiveCharacterTextSplitter(chunk_size=embed_chunk_size, chunk_overlap=embed_overlap)  \n",
    "            # docs = loader.load()\n",
    "            # subdocs = text_splitter.split_documents(docs)\n",
    "            # #\n",
    "            # print(subdocs[0].page_content)\n",
    "            # #embeddings = [embeddings_model.embed_query(text.page_content) for text in subdocs]\n",
    "            # #print(embeddings)\n",
    "            # print(\"Adding items into the Chroma database\")\n",
    "            # vectorstore = Chroma.from_documents(subdocs, embeddings_model, collection_name=rag_index_name)\n",
    "elif use_Pinecone:\n",
    "    print(f'initializing the Pinecone vector store {rag_index_name}')\n",
    "\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    import time\n",
    "    \n",
    "    if rag_index_name in pc.list_indexes().names():\n",
    "        print(\"Index {} already created\".format(rag_index_name))\n",
    "        #exit()\n",
    "        index = pc.Index(rag_index_name)\n",
    "    else:\n",
    "        print(\"Creating the Pinecone index...\")\n",
    "        pc.create_index(\n",
    "            name=rag_index_name,\n",
    "            dimension=embeddings_vector_size,\n",
    "            metric='euclidean',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "        # Wait for index to be initialized\n",
    "        while not pc.describe_index(rag_index_name).status[\"ready\"]:\n",
    "            # TODO remove me\n",
    "            print(\"sleeping\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        if not use_Ollama_For_Embedding:\n",
    "            index = VectorstoreIndexCreator(\n",
    "                vectorstore_cls=PineconeVectorStore,\n",
    "                embedding=embeddings_model,\n",
    "                text_splitter=rc_text_splitter,\n",
    "                vectorstore_kwargs={ \"index_name\": rag_index_name}\n",
    "            ).from_loaders([loader])\n",
    "        else:\n",
    "            print('using Ollama embeddings')\n",
    "            \n",
    "            \n",
    "            docs = loader.load()\n",
    "            subdocs = rc_text_splitter.split_documents(docs)\n",
    "            #\n",
    "            #print(subdocs[0].page_content)\n",
    "            #embeddings = [embeddings_model.embed_query(text.page_content) for text in subdocs]\n",
    "            #print(embeddings)\n",
    "            print(\"Upserting the vectors into the database\")\n",
    "            vectorstore = PineconeVectorStore.from_documents(subdocs, embeddings_model, index_name=rag_index_name)\n",
    "        \n",
    "        #vectorstore.add_documents(splits)\n",
    "else:\n",
    "    print('using DocArrayInMemorySearch')\n",
    "    index = VectorstoreIndexCreator(\n",
    "        vectorstore_cls=DocArrayInMemorySearch,\n",
    "        embedding=embeddings_model\n",
    "    ).from_loaders([loader])\n",
    "\n",
    "    inMemoryIndex = index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaClient = chromadb.PersistentClient(path=storage_path)\n",
    "print(chromaClient.count_collections())\n",
    "print(chromaClient.list_collections())\n",
    "\n",
    "#rag_index_name = 'langchain-deeplearningai-nomic-ollama'\n",
    "print(rag_index_name)\n",
    "collection = chromaClient.get_collection(name=rag_index_name)\n",
    "#collection.modify(name=rag_index_name + \"_v0\")\n",
    "print(collection.get(\n",
    "    include=['embeddings', 'documents', 'metadatas']\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b8fc1",
   "metadata": {},
   "source": [
    "# Part 2: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f22cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:29:11 - INFO - segment.py:create_collection:189 - Collection langchain-deeplearningai-nomic-ollama is not created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Chroma for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12,257 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:12 - DEBUG - connectionpool.py:_make_request:549 - https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "if use_Chroma:\n",
    "    print(\"using Chroma for inference\")\n",
    "    db = Chroma(collection_name=rag_index_name, embedding_function=embeddings_model, persist_directory=storage_path)\n",
    "elif use_Pinecone:\n",
    "    print(\"using Pinecone for inference\")\n",
    "    db = PineconeVectorStore(index=pc.Index(rag_index_name), embedding=embeddings_model, text_key=\"text\")\n",
    "else:\n",
    "    print(\"using DocArrayInMemorySearch for inference\")\n",
    "    # db = DocArrayInMemorySearch.from_documents(\n",
    "    #     docs, \n",
    "    #     embeddings_model\n",
    "    # )\n",
    "    db = inMemoryIndex.vectorstore\n",
    "\n",
    "def create_QA_chain(llm, max_doc_count):\n",
    "\n",
    "    system_prompt = (\n",
    "        \"Use the given context to answer the question. \"\n",
    "        \"If you don't know the answer, say you don't know. \"\n",
    "        \"Use a maximum of five sentences and keep the answer concise. \"\n",
    "        \"Context: {context}\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retriever = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": max_doc_count}\n",
    "    )\n",
    "    \n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    return chain\n",
    "\n",
    "def process_query(query, llm, max_doc_count=5):\n",
    "    print(query)\n",
    "    print(llm)\n",
    "\n",
    "    if use_Ollama_For_Inference:\n",
    "        # if use_Pinecone:\n",
    "\n",
    "        #     from langchain.chains import RetrievalQAWithSourcesChain\n",
    "        #     text_field = \"text\"  # the metadata field that contains our text\n",
    "        #     index = pc.Index(pinecone_index_name)\n",
    "        #     vectorstore = PineconeVectorStore(\n",
    "        #         index, embeddings_model, text_field\n",
    "        #     )\n",
    "        # else:\n",
    "        #     vectorstore = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "\n",
    "        # Retrieval Q&A chain with sources\n",
    "        # We do similarity search and return the top 5 results\n",
    "        #if use_Pinecone:\n",
    "            #vectorstore = PineconeVectorStore(\n",
    "            #index=index, embedding=embeddings_model)\n",
    "            \n",
    "            # qa = RetrievalQA.from_chain_type(\n",
    "            #     llm=ollama,\n",
    "            #     retriever=vectorstore.as_retriever(\n",
    "            #         search_type=\"similarity\",\n",
    "            #         search_kwargs={\"k\": 5}\n",
    "            #     )\n",
    "            # )\n",
    "        #else:\n",
    "            #vectorstore = index.vectorstore\n",
    "            # from langchain import PromptTemplate\n",
    "            # # Prompt\n",
    "            # template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "            # If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.\n",
    "            # Use three sentences maximum and keep the answer as concise as possible.\n",
    "            # {context}\n",
    "            # Question: {question}\n",
    "            # Helpful Answer:\"\"\"\n",
    "\n",
    "            # QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],\n",
    "            # template=template)\n",
    "            # qa = RetrievalQA.from_chain_type(llm=ollama,\n",
    "            # retriever=vectorstore.as_retriever(),\n",
    "            # chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "        qa = create_QA_chain(llm, max_doc_count)\n",
    "        print(qa)\n",
    "        \n",
    "        return qa.invoke({\"input\": query})\n",
    "        # else:\n",
    "        #     print(\"sorry, Ollama doesn't yet support the /v1/completions API and thus cannot be used with the index.query() method\" )\n",
    "    else:\n",
    "        return index.query(question=query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "34562d81",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please list all your shirts with sun protection in a table in markdown and summarize each one.'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"Please list all your shirts with sun protection in a table in markdown and summarize each one.\"\n",
    "if use_Test_Data:\n",
    "    query =\"Please tell me about Nautical Navy Patio Settee. Provide the response in markdown.\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b26c3388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:29:32 - DEBUG - _config.py:load_ssl_context:80 - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-06-10 10:29:32 - DEBUG - _config.py:load_ssl_context_verify:146 - load_verify_locations cafile='h:\\\\Users\\\\Raphael\\\\OneDrive\\\\Perso\\\\Technical\\\\AI\\\\Training\\\\GenAI\\\\LangChain-for-LLM-Application-Development\\\\venv\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "2024-06-10 10:29:32 - DEBUG - _config.py:load_ssl_context:80 - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-06-10 10:29:32 - DEBUG - _config.py:load_ssl_context_verify:146 - load_verify_locations cafile='h:\\\\Users\\\\Raphael\\\\OneDrive\\\\Perso\\\\Technical\\\\AI\\\\Training\\\\GenAI\\\\LangChain-for-LLM-Application-Development\\\\venv\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32,611 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:29:32 - DEBUG - connectionpool.py:_new_conn:244 - Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please list all your shirts with sun protection in a table in markdown and summarize each one.\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000001FBDA8EE1A0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001FBDA8D2DA0> model_name='gemma:2b' temperature=0.0 openai_api_key=SecretStr('**********') openai_api_base='http://localhost:11434/v1' openai_proxy=''\n",
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001FBDA8D15A0>, search_kwargs={'k': 5}), config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the given context to answer the question. If you don't know the answer, say you don't know. Use a maximum of five sentences and keep the answer concise. Context: {context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001FBDA8EE1A0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001FBDA8D2DA0>, model_name='gemma:2b', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='http://localhost:11434/v1', openai_proxy='')\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36,595 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36 - DEBUG - connectionpool.py:_make_request:549 - http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:29:36 - DEBUG - _base_client.py:_build_request:446 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Use the given context to answer the question. If you don\\'t know the answer, say you don\\'t know. Use a maximum of five sentences and keep the answer concise. Context: : 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\r\\n\\r\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\r\\n\\r\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\r\\n\\r\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\r\\n\\r\\nSun Protection That Won\\'t Wear Off\\r\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.\\n\\n: 709\\nname: Sunrise Tee\\ndescription: Stay cool, comfortable and dry on the hottest days in our women\\'s UV-protective button down shirt. The lightweight, high-performance fabric wicks away moisture and dries quickly.\\r\\n\\r\\nSize & Fit\\r\\nSlightly Fitted: Softly shapes the body. Falls at hip.\\r\\n\\r\\nWhy We Love It\\r\\nOur lightest hot-weather shirt lets you beat the heat. Originally designed for fishing, it\\'s also a great choice for travel thanks to its wrinkle-free fabric and built-in sun protection with a rating of UPF 50+.\\r\\n\\r\\nFabric & Care\\r\\nLightweight performance synthetic wicks moisture, resists wrinkles and dries fast. Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Machine wash and dry.\\r\\n\\r\\nAdditional Features\\r\\nBuilt-in SunSmart™ UPF 50+ rated – the highest rated sun protection possible. The high-performance fabric keeps you cool and comfortable by wicking perspiration away. Smoother buttons, low-profile pockets and side shaping for a flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Wrinkle free. Imported.\\r\\n\\r\\nSun Protection That Won\\'t Wear Off\\r\\nOur\\n\\n: 679\\nname: Women\\'s Tropical Tee, Sleeveless\\ndescription: Our five-star sleeveless button-up shirt has a fit to flatter and SunSmart™ protection to block the sun’s harmful UV rays. Size & Fit: Slightly Fitted: Softly shapes the body. Falls at hip. Fabric & Care: Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Built-in SunSmart™ UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Updated design with smoother buttons. Wrinkle resistant. Low-profile pockets and side shaping offer a more flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Imported. Sun Protection That Won\\'t Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays.\\n\\n: 374\\nname: Men\\'s Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun\\'s harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\\n\\n: 915\\nname: Serene Sun Hat\\ndescription: From the beach to the garden, this paper sun hat has an extra-wide brim for plenty of sun protection and effortless style. With 100% paper construction and UPF 50+ rated – the highest rated sun protection possible – this hat is the perfect choice for a sunny day. Spot clean for easy maintenance. Enjoy a slightly floppy look for a fun and stylish look. Imported. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. Sun protection that won\\'t wear off!', 'role': 'system'}, {'content': 'Please list all your shirts with sun protection in a table in markdown and summarize each one.', 'role': 'user'}], 'model': 'gemma:2b', 'n': 1, 'stream': False, 'temperature': 0.0}}\n",
      "2024-06-10 10:29:36 - DEBUG - _base_client.py:_request:949 - Sending HTTP Request: POST http://localhost:11434/v1/chat/completions\n",
      "2024-06-10 10:29:36 - DEBUG - _trace.py:trace:45 - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36,969 DEBUG https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:36 - DEBUG - connectionpool.py:_make_request:549 - https://us-api.i.posthog.com:443 \"POST /batch/ HTTP/1.1\" 200 15\n",
      "2024-06-10 10:29:38 - DEBUG - _trace.py:trace:45 - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FBDA2F8F10>\n",
      "2024-06-10 10:29:38 - DEBUG - _trace.py:trace:45 - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-06-10 10:29:38 - DEBUG - _trace.py:trace:45 - send_request_headers.complete\n",
      "2024-06-10 10:29:38 - DEBUG - _trace.py:trace:45 - send_request_body.started request=<Request [b'POST']>\n",
      "2024-06-10 10:29:38 - DEBUG - _trace.py:trace:45 - send_request_body.complete\n",
      "2024-06-10 10:29:38 - DEBUG - _trace.py:trace:45 - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-06-10 10:29:45 - DEBUG - _trace.py:trace:45 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Date', b'Mon, 10 Jun 2024 08:29:45 GMT'), (b'Content-Length', b'808')])\n",
      "2024-06-10 10:29:45 - INFO - _client.py:_send_single_request:1026 - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-10 10:29:45 - DEBUG - _trace.py:trace:45 - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-06-10 10:29:45 - DEBUG - _trace.py:trace:45 - receive_response_body.complete\n",
      "2024-06-10 10:29:45 - DEBUG - _trace.py:trace:45 - response_closed.started\n",
      "2024-06-10 10:29:45 - DEBUG - _trace.py:trace:45 - response_closed.complete\n",
      "2024-06-10 10:29:45 - DEBUG - _base_client.py:_request:988 - HTTP Response: POST http://localhost:11434/v1/chat/completions \"200 OK\" Headers({'content-type': 'application/json', 'date': 'Mon, 10 Jun 2024 08:29:45 GMT', 'content-length': '808'})\n",
      "2024-06-10 10:29:45 - DEBUG - _base_client.py:_request:996 - request_id: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Please list all your shirts with sun protection in a table in markdown and summarize each one.', 'context': [Document(page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\r\\n\\r\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\r\\n\\r\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\r\\n\\r\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\r\\n\\r\\nSun Protection That Won\\'t Wear Off\\r\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.', metadata={'row': 255, 'source': 'OutdoorClothingCatalog_1000.csv'}), Document(page_content=\": 709\\nname: Sunrise Tee\\ndescription: Stay cool, comfortable and dry on the hottest days in our women's UV-protective button down shirt. The lightweight, high-performance fabric wicks away moisture and dries quickly.\\r\\n\\r\\nSize & Fit\\r\\nSlightly Fitted: Softly shapes the body. Falls at hip.\\r\\n\\r\\nWhy We Love It\\r\\nOur lightest hot-weather shirt lets you beat the heat. Originally designed for fishing, it's also a great choice for travel thanks to its wrinkle-free fabric and built-in sun protection with a rating of UPF 50+.\\r\\n\\r\\nFabric & Care\\r\\nLightweight performance synthetic wicks moisture, resists wrinkles and dries fast. Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Machine wash and dry.\\r\\n\\r\\nAdditional Features\\r\\nBuilt-in SunSmart™ UPF 50+ rated – the highest rated sun protection possible. The high-performance fabric keeps you cool and comfortable by wicking perspiration away. Smoother buttons, low-profile pockets and side shaping for a flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Wrinkle free. Imported.\\r\\n\\r\\nSun Protection That Won't Wear Off\\r\\nOur\", metadata={'row': 709, 'source': 'OutdoorClothingCatalog_1000.csv'}), Document(page_content=\": 679\\nname: Women's Tropical Tee, Sleeveless\\ndescription: Our five-star sleeveless button-up shirt has a fit to flatter and SunSmart™ protection to block the sun’s harmful UV rays. Size & Fit: Slightly Fitted: Softly shapes the body. Falls at hip. Fabric & Care: Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Built-in SunSmart™ UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Updated design with smoother buttons. Wrinkle resistant. Low-profile pockets and side shaping offer a more flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Imported. Sun Protection That Won't Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\", metadata={'row': 679, 'source': 'OutdoorClothingCatalog_1000.csv'}), Document(page_content=\": 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\", metadata={'row': 374, 'source': 'OutdoorClothingCatalog_1000.csv'}), Document(page_content=\": 915\\nname: Serene Sun Hat\\ndescription: From the beach to the garden, this paper sun hat has an extra-wide brim for plenty of sun protection and effortless style. With 100% paper construction and UPF 50+ rated – the highest rated sun protection possible – this hat is the perfect choice for a sunny day. Spot clean for easy maintenance. Enjoy a slightly floppy look for a fun and stylish look. Imported. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays. Sun protection that won't wear off!\", metadata={'row': 915, 'source': 'OutdoorClothingCatalog_1000.csv'})], 'answer': \"| Name | Description |\\n|---|---|\\n| Sun Shield Shirt by 255 | SPF 50+ sun protection, lightweight, soft, and wicks moisture |\\n| Sunrise Tee | UPF 50+ sun protection, lightweight, wicks moisture, and dries quickly |\\n| Women's Tropical Tee, Sleeveless | SPF 50+ sun protection, UPF 50+ coverage, and is wrinkle-free |\\n| Men's Plaid Tropic Shirt, Short-Sleeve | SPF 50+ sun protection, UPF 50+ coverage, and is wrinkle-free |\\n| Serene Sun Hat | SPF 50+ sun protection, extra-wide brim, and is perfect for a sunny day |\"}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Name | Description |\n",
       "|---|---|\n",
       "| Sun Shield Shirt by 255 | SPF 50+ sun protection, lightweight, soft, and wicks moisture |\n",
       "| Sunrise Tee | UPF 50+ sun protection, lightweight, wicks moisture, and dries quickly |\n",
       "| Women's Tropical Tee, Sleeveless | SPF 50+ sun protection, UPF 50+ coverage, and is wrinkle-free |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | SPF 50+ sun protection, UPF 50+ coverage, and is wrinkle-free |\n",
       "| Serene Sun Hat | SPF 50+ sun protection, extra-wide brim, and is perfect for a sunny day |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0.0, base_url=inferApiBase, model=infer_model_name)\n",
    "response = process_query(query=query, llm=llm)\n",
    "print(response)\n",
    "display(Markdown(response['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0, api_key=openai.api_key, base_url=openai.api_base, model=infer_model_name)\n",
    "\n",
    "# from langchain import PromptTemplate\n",
    "# vectorstore = index.vectorstore\n",
    "# # Prompt\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "# If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.\n",
    "# Use three sentences maximum and keep the answer as concise as possible.\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "\n",
    "# QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],\n",
    "# template=template)\n",
    "# messages = QA_CHAIN_PROMPT.format_messages(question=query)\n",
    "# qa = RetrievalQA.from_chain_type(llm=ollama,\n",
    "# retriever=vectorstore.as_retriever(),\n",
    "# chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "# qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356663a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import PromptTemplate\n",
    "#             # Prompt\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "# If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.\n",
    "# Use three sentences maximum and keep the answer as concise as possible.\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "# if use_Pinecone:\n",
    "#     vectorstore = PineconeVectorStore(\n",
    "#             index=index, embedding=embeddings_model\n",
    "#         )\n",
    "\n",
    "# from langchain.llms import Ollama\n",
    "# ollama = Ollama(model=infer_model_name)\n",
    "# QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],\n",
    "# template=template)\n",
    "# qa = RetrievalQA.from_chain_type(llm=ollama,\n",
    "# retriever=vectorstore.as_retriever(),\n",
    "# chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "# qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682bc47e",
   "metadata": {},
   "source": [
    "#### Comparing the Ollama models\n",
    "\n",
    "| LLM          | Inference Time | Sample Output                             |\n",
    "|--------------|----------------|-------------------------------------------|\n",
    "| Gemma 2B     | 12s           | Sure, here's the answer:\\n\\nNautical Navy Patio Settee is a comfortable and weather-resistant outdoor furniture set made in the USA. It is designed to withstand harsh weather conditions and is perfect for relaxing on your patio or roof.|\n",
    "| Gemma 7B     | 18s           | The provided text does not contain any information regarding the price of the Nautical Navy Patio Settee, so I am unable to answer this question from the given context.         |\n",
    "| Cohere Aya 8B| 45s           | # Nautical Navy Patio Settee \\n\\n## Description: \\nThe Nautical Navy Patio Settee is an outdoor furniture piece designed for comfort and durability. Crafted in the USA, it is made to withstand various weather conditions without warping, cracking, or rotting. The settee coordinates with the All-Weather Collection and offers low maintenance. \\n\\n## Dimensions: \\n- Height: 32¾\" \\n- Width: 51½\" \\n- Depth: 33¼\" \\n\\n## Additional Features: \\n- Made from low-maintenance HDPE material. \\n- Sunbrella cushion with a built-in drain is mildew and mold-resistant, ensuring ease of cleaning. \\n- Easy to assemble and made in the USA. |\n",
    "| Llama3 8B| 44s           | **Nautical Navy Patio Settee**\\n============================\\n\\nEnjoy outdoor relaxation with our comfortable **All-Weather Settee**. Crafted with care in the USA, it is designed to provide years of comfort with low maintenance and coordinates perfectly with our All-Weather Collection.\\n\\n**Specs**\\n--------\\n\\n* Dimensions: 32¾\"H x 51½\"W x 33¼\"D\\n\\n**Why We Love It**\\n-------------------\\n\\nOur All-Weather Furniture has been tested under punishing New England weather conditions at our company headquarters in Freeport, Maine for a year. Despite being fully exposed, it won’t rot, splinter, warp or crack, making it perfect for your lawn, patio or roof.\\n\\n**Construction**\\n--------------\\n\\n* Love seat made from low-maintenance High-Density Polyethylene (HDPE)\\n* Cushion covered with mildew- and mold-resistant Sunbrella cushion with built-in drain\\n\\n**Additional Features**\\n----------------------\\n\\n* Will not splinter, crack, rot or warp for years of hassle-free use\\n* Love seat is super easy to clean with soap and water; spot clean the cushion\\n* Assembly required\\n* Made in the USA |\n",
    "| Phi3 3.8B| 17s           | # Nautical Navy Patio Settee\\n\\nThe **Nautical Navy Patio Settee** is an outdoor furniture piece designed for comfortable relaxation under various weather conditions. It features:\\n\\n- Dimensions: 32¾\"H x 51½\"W x 33¼\"D\\n\\n- Made in the USA with low maintenance and durability as key attributes.\\n\\n- Constructed from High-Density Polyethylene (HDPE) for longevity, resisting splintering, cracking, rotting, or warping.\\n\\n- A mildew- and mold-resistant Sunbrella cushion with a built-in drain system.\\n\\n- Easy to clean using soap and water; spot clean the cushion as needed.\\n\\n- Requires assembly upon purchase. |\n",
    "| Mistral 7B| 24s           | The Nautical Navy Patio Settee is a comfortable, all-weather love seat designed for outdoor relaxation. It is crafted with low-maintenance High-Density Polyethylene (HDPE) and features a Sunbrella cushion that is resistant to mildew and mold. The settee has been tested under harsh New England weather conditions and is known for its durability, as it won't rot, splinter, warp or crack. It coordinates perfectly with the All-Weather Collection and can be easily cleaned with soap and water. Assembly is required, and it is made in the USA.The Nautical Navy Patio Settee is a comfortable, all-weather love seat designed for outdoor relaxation. It is crafted with low-maintenance High-Density Polyethylene (HDPE) and features a Sunbrella cushion that is resistant to mildew and mold. The settee has been tested under harsh New England weather conditions and is known for its durability, as it won't rot, splinter, warp or crack. It coordinates perfectly with the All-Weather Collection and can be easily cleaned with soap and water. Assembly is required, and it is made in the USA. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "52847ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:34:13 - DEBUG - _config.py:load_ssl_context:80 - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-06-10 10:34:13 - DEBUG - _config.py:load_ssl_context_verify:146 - load_verify_locations cafile='h:\\\\Users\\\\Raphael\\\\OneDrive\\\\Perso\\\\Technical\\\\AI\\\\Training\\\\GenAI\\\\LangChain-for-LLM-Application-Development\\\\venv\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "2024-06-10 10:34:13 - DEBUG - _config.py:load_ssl_context:80 - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-06-10 10:34:13 - DEBUG - _config.py:load_ssl_context_verify:146 - load_verify_locations cafile='h:\\\\Users\\\\Raphael\\\\OneDrive\\\\Perso\\\\Technical\\\\AI\\\\Training\\\\GenAI\\\\LangChain-for-LLM-Application-Development\\\\venv\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13,319 DEBUG Starting new HTTP connection (1): localhost:11434\n",
      "2024-06-10 10:34:13 - DEBUG - connectionpool.py:_new_conn:244 - Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please describe the Women's Campside Oxfords product. Provide the response in markdown format. Take great attention to remove any '```markdown\n",
      "' keyword that may be at the beginning of your response.\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000001FBDA8D1510> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001FBDA2FA530> model_name='phi3:3.8b' temperature=0.0 openai_api_key=SecretStr('**********') openai_api_base='http://localhost:11434/v1' openai_proxy=''\n",
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001FBDA8D15A0>, search_kwargs={'k': 1}), config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the given context to answer the question. If you don't know the answer, say you don't know. Use a maximum of five sentences and keep the answer concise. Context: {context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001FBDA8D1510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001FBDA2FA530>, model_name='phi3:3.8b', temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='http://localhost:11434/v1', openai_proxy='')\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17,097 DEBUG http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17 - DEBUG - connectionpool.py:_make_request:549 - http://localhost:11434 \"POST /api/embeddings HTTP/1.1\" 200 None\n",
      "2024-06-10 10:34:17 - DEBUG - _base_client.py:_build_request:446 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': \"Use the given context to answer the question. If you don't know the answer, say you don't know. Use a maximum of five sentences and keep the answer concise. Context: : 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\r\\n\\r\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\r\\n\\r\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\r\\n\\r\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\r\\n\\r\\nQuestions? Please contact us for any inquiries.\", 'role': 'system'}, {'content': \"Please describe the Women's Campside Oxfords product. Provide the response in markdown format. Take great attention to remove any '```markdown\\n' keyword that may be at the beginning of your response.\", 'role': 'user'}], 'model': 'phi3:3.8b', 'n': 1, 'stream': False, 'temperature': 0.0}}\n",
      "2024-06-10 10:34:17 - DEBUG - _base_client.py:_request:949 - Sending HTTP Request: POST http://localhost:11434/v1/chat/completions\n",
      "2024-06-10 10:34:17 - DEBUG - _trace.py:trace:45 - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None\n",
      "2024-06-10 10:34:19 - DEBUG - _trace.py:trace:45 - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FBE3FA5750>\n",
      "2024-06-10 10:34:19 - DEBUG - _trace.py:trace:45 - send_request_headers.started request=<Request [b'POST']>\n",
      "2024-06-10 10:34:19 - DEBUG - _trace.py:trace:45 - send_request_headers.complete\n",
      "2024-06-10 10:34:19 - DEBUG - _trace.py:trace:45 - send_request_body.started request=<Request [b'POST']>\n",
      "2024-06-10 10:34:19 - DEBUG - _trace.py:trace:45 - send_request_body.complete\n",
      "2024-06-10 10:34:19 - DEBUG - _trace.py:trace:45 - receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-06-10 10:34:33 - DEBUG - _trace.py:trace:45 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json'), (b'Date', b'Mon, 10 Jun 2024 08:34:33 GMT'), (b'Content-Length', b'1212')])\n",
      "2024-06-10 10:34:33 - INFO - _client.py:_send_single_request:1026 - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-10 10:34:33 - DEBUG - _trace.py:trace:45 - receive_response_body.started request=<Request [b'POST']>\n",
      "2024-06-10 10:34:33 - DEBUG - _trace.py:trace:45 - receive_response_body.complete\n",
      "2024-06-10 10:34:33 - DEBUG - _trace.py:trace:45 - response_closed.started\n",
      "2024-06-10 10:34:33 - DEBUG - _trace.py:trace:45 - response_closed.complete\n",
      "2024-06-10 10:34:33 - DEBUG - _base_client.py:_request:988 - HTTP Response: POST http://localhost:11434/v1/chat/completions \"200 OK\" Headers({'content-type': 'application/json', 'date': 'Mon, 10 Jun 2024 08:34:33 GMT', 'content-length': '1212'})\n",
      "2024-06-10 10:34:33 - DEBUG - _base_client.py:_request:996 - request_id: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Please describe the Women's Campside Oxfords product. Provide the response in markdown format. Take great attention to remove any '```markdown\\n' keyword that may be at the beginning of your response.\",\n",
       " 'context': [Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\r\\n\\r\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\r\\n\\r\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\r\\n\\r\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\r\\n\\r\\nQuestions? Please contact us for any inquiries.\", metadata={'row': 0, 'source': 'OutdoorClothingCatalog_1000.csv'})],\n",
       " 'answer': \" # Women's Campside Oxfords\\n\\nThe **Women's Campside Oxfords** are ultra-comfortable shoes featuring a lace-to-toe design with super-soft canvas material and thick cushioning for an immediate comfortable fit upon first use. They come in regular shoe sizes, but if half sizes aren't available, the next whole size up is recommended.\\n\\n## Size & Fit:\\n- **Order Regular Shoe Size**\\n- For unavailable half sizes, order one size larger.\\n\\n## Specifications:\\n- **Approximate Weight per Pair:** 1 lb. 1 oz.\\n- **Construction Details:**\\n  - Made from soft canvas material for a comfortable break-in experience.\\n  - Innersole with Cleansport NXT® antimicrobial odor control and vintage hunt, fish, and camping motif.\\n  - Moderate arch contour in the innersole.\\n  - EVA foam midsole for cushioning and support.\\n  - Chain-tread-inspired molded rubber outsole with a modified chain-tread pattern, imported from overseas.\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " # Women's Campside Oxfords\n",
       "\n",
       "The **Women's Campside Oxfords** are ultra-comfortable shoes featuring a lace-to-toe design with super-soft canvas material and thick cushioning for an immediate comfortable fit upon first use. They come in regular shoe sizes, but if half sizes aren't available, the next whole size up is recommended.\n",
       "\n",
       "## Size & Fit:\n",
       "- **Order Regular Shoe Size**\n",
       "- For unavailable half sizes, order one size larger.\n",
       "\n",
       "## Specifications:\n",
       "- **Approximate Weight per Pair:** 1 lb. 1 oz.\n",
       "- **Construction Details:**\n",
       "  - Made from soft canvas material for a comfortable break-in experience.\n",
       "  - Innersole with Cleansport NXT® antimicrobial odor control and vintage hunt, fish, and camping motif.\n",
       "  - Moderate arch contour in the innersole.\n",
       "  - EVA foam midsole for cushioning and support.\n",
       "  - Chain-tread-inspired molded rubber outsole with a modified chain-tread pattern, imported from overseas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_model_name = \"gemma:2b\" #12s\n",
    "#infer_model_name = \"gemma:7b\" #18s\n",
    "#infer_model_name = \"aya:8b\" #45s\n",
    "#infer_model_name = \"llama3:8b\" #44s\n",
    "infer_model_name = \"phi3:3.8b\" #17s\n",
    "#infer_model_name = \"mistral:7b\" #24s\n",
    "\n",
    "optional_instruction = ''\n",
    "if infer_model_name == \"phi3:3.8b\":\n",
    "    optional_instruction = \". Take great attention to remove any '```markdown\\n' keyword that may be at the beginning of your response\"\n",
    "query =f\"Please describe the Women's Campside Oxfords product. Provide the response in markdown format{optional_instruction}.\"\n",
    "#query = f'Please tell me about the Nautical Navy Patio Settee. Provide the response in markdown format{optional_instruction}.'\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0.0, base_url=inferApiBase, model=infer_model_name)\n",
    "#llm = ChatOpenAI(temperature = 0.0)\n",
    "\n",
    "response = process_query(query=query, llm=llm, max_doc_count=1)\n",
    "display(response)\n",
    "display(Markdown(response['answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919492d5",
   "metadata": {},
   "source": [
    "## OpenAI compatibility playground code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_Ollama_For_Inference:\n",
    "    from openai import OpenAI\n",
    "\n",
    "    chromaClient = OpenAI(\n",
    "        base_url = 'http://localhost:11434/v1/',\n",
    "        api_key='ollama', # required, but unused\n",
    "    )\n",
    "    chat_completion = chromaClient.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\r\\n\\r\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\r\\n\\r\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\r\\n\\r\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\r\\n\\r\\nQuestions? Please contact us for any inquiries.\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Please describe the Women\\'s Campside Oxfords product. Please provide the answer in Markdown format.'\n",
    "            }\n",
    "        ],\n",
    "        model='gemma:7b',\n",
    "        stream=False,\n",
    "        temperature=0.0,\n",
    "        n=1\n",
    "    )\n",
    "    display(Markdown(chat_completion.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761aea84",
   "metadata": {},
   "source": [
    "#### Comparing the ChatOpenAI (with Ollama bindings) and Ollama LangChain objects\n",
    "Conclusion: not much difference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "ollama = Ollama(model=infer_model_name, \n",
    "                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "                )\n",
    "response = process_query(query=query, llm=ollama, max_doc_count=1)\n",
    "display(response)\n",
    "display(Markdown(response['answer']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb235a0",
   "metadata": {},
   "source": [
    "#### Testing the ollama library\n",
    "Points to note:\n",
    "    - The test is using the previous response, so you need to perform the previous test first\n",
    "    - I haven't found a way to output any log\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response['context'][0].page_content\n",
    "prompt = response['input']\n",
    "\n",
    "import ollama\n",
    "output = ollama.generate(\n",
    "  model=\"gemma:2b\",\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\"\n",
    ")\n",
    "\n",
    "display(Markdown(output['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631396c6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=file, encoding='utf-8')\n",
    "if use_Ollama_For_Inference:\n",
    "    loader = LimitedCSVLoader(file_path=file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2164b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a977f44",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875693a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "if(use_Ollama_For_Inference):\n",
    "    embeddings_model = OllamaEmbeddings(model=embeddings_model_name, embed_instruction='', query_instruction='')\n",
    "else:\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad0bb0",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "if use_Pinecone:\n",
    "    db = PineconeVectorStore(\n",
    "        index=pc.Index(rag_index_name), embedding=embeddings_model, text_key=\"text\"\n",
    "    )\n",
    "else:\n",
    "\n",
    "    db = DocArrayInMemorySearch.from_documents(\n",
    "        docs, \n",
    "        embeddings_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329bfd5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2acd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please tell me about Nautical Navy Patio Settee\"\n",
    "query = \"Enduring Outdoor Chair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed = embeddings_model.embed_query(query)\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c6b7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "docs = db.similarity_search_with_relevance_scores(query=query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43321853",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba90b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625f5e8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0, base_url=openai.api_base, model=infer_model_name)\n",
    "#llm = ChatOpenAI(temperature = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573f58a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14682d95",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "response = llm.invoke(f\"{qdocs} Question: Please tell me about Nautical Navy Patio Settee. Please provide the answer in Markdown format but do not include the 'markdown' string\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c280c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba545b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b338f",
   "metadata": {},
   "source": [
    "### Deprecation Notice\n",
    "The following piece of code is soon deprecated and doesn't work with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c94d22",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#response = qa_stuff.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1a45a",
   "metadata": {},
   "source": [
    "### Code that works\n",
    "This piece of code works with both Ollama and OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use a maximum of 5 sentences and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4769316",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\"\n",
    "query = \"Please tell me about Nautical Navy Patio Settee. Please provide the answer in Markdown format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1a5db",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "display(Markdown(response['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ec062",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffb19f",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b58916",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
